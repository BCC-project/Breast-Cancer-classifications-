# -*- coding: utf-8 -*-
"""pyhton new project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hG4QQAz5yP6JoqOsU5ZciVZTjXGrD2Pi
"""

# Import important libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Loading the dataset
data = pd.read_csv("breast_cancer_data.csv")

# getting some basic datainfo
data.head(5)

data.info()

# Id column is redundant and not useful, we want to drop it
data.drop('id', axis =1, inplace=True)
data.head(2)

data.shape

data.info()

data.describe()

# Review number of columns of each data type in a DataFrame:
data.dtypes.value_counts()

# To count how many times each unique row appears in the DataFrame "data"
data.value_counts()

# checking the missing values
data.isnull().sum()

data.diagnosis.unique()

data['diagnosis'].value_counts()

data.head(5)

data.keys()

data['diagnosis'] = data['diagnosis'].replace({'M': 1, 'B': 0})

data['diagnosis'].value_counts()

# Set plot style and context
sns.set_style("white")
sns.set_context({"figure.figsize": (10, 8)})

# Create count plot with 'diagnosis' as x-axis
sns.countplot(x='diagnosis', data=data, palette="Set3").set(title='Cancer Diagnosis Count')

data.corr()

numerical_columns=data.columns[data.dtypes!='object']

plt.figure(figsize=(30,30))
sns.heatmap(data[numerical_columns].corr(),annot=True,fmt = ".2f")

sns.pairplot(data[numerical_columns],hue='diagnosis')

#Distribution of the data
plt.figure(figsize=(8,6))
x=0
for i in numerical_columns:
    sns.histplot(data=data,x=i,kde=True)
    print('\n')
    plt.show()

X=data.drop('diagnosis', axis=1)
y=data['diagnosis']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train ,y_test =train_test_split(X,y, test_size=0.2, random_state=0)

# scaling data
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

X_train.shape, X_test.shape,y_train.shape,y_test.shape

"""**Model Training Using Logistic Regression**"""

from sklearn.linear_model import LogisticRegression
#Initilize Logistic Regression
log = LogisticRegression()

# Train the model
log.fit(X_train,y_train)

# Model Evaluation on training data
log.score(X_train,y_train)

# Predict using the trained model
y_pred1 = log.predict(X_test)

y_pred1

# Model Evaluation
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc, log_loss

# Calculate accuracy score
accuracy_log = accuracy_score(y_test, y_pred1)
print("Accuracy:", accuracy_log)

# Classification Report
print(classification_report(y_test, y_pred1))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred1)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# ROC Curve and AUC
fpr, tpr, thresholds = roc_curve(y_test, y_pred1)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], 'r--')  # Diagonal line
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend(loc="lower right")
plt.show()

# Log Loss
logloss = log_loss(y_test, y_pred1)
print(f"Log Loss: {logloss:.4f}")

"""**Model Training using Random Forest Classifier**"""

from sklearn.ensemble import RandomForestClassifier
# Initilize RandomForestClassifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
rf.fit(X_train, y_train)

# Model Evaluation on trained data
rf.score(X_train, y_train)

# Make predictions
y_pred2 = rf.predict(X_test)

y_pred2

# Model Evaluation
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
# Calculate accuracy score
accuracy_ranf = accuracy_score(y_test, y_pred2)
print("Accuracy:", accuracy_ranf)

# Classification Report
print(classification_report(y_test, y_pred2))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred2)
sns.heatmap(cm, annot=True, fmt='d', cmap='Reds')
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# ROC Curve and AUC
fpr, tpr, thresholds = roc_curve(y_test, y_pred2)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], 'r--')  # Diagonal line
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend(loc="lower right")
plt.show()

# performing cross validation
from sklearn.model_selection import cross_val_score
cv_scores = cross_val_score(rf, X, y, cv=5)
print("Cross-Validation Scores:", cv_scores)
print("Mean CV Accuracy:", cv_scores.mean())

from sklearn.metrics import roc_curve, auc
from sklearn.metrics import roc_auc_score
y_proba = rf.predict_proba(X_test)[:, 1]
fpr, tpr, thresholds = roc_curve(y_test, y_proba)
auc_score = auc(fpr, tpr)
print("AUC Score:", auc_score)

"""**Model Training using KNN**"""

# KNN
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier()
knn.fit(X_train, y_train)

# Model Evaluation on training data
knn.score(X_train, y_train)

y_pred3 = knn.predict(X_test)

y_pred3

# Model Evaluation
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
# Calculate accuracy score
accuracy_knn = accuracy_score(y_test, y_pred3)
print("Accuracy:", accuracy_knn)

# Classification Report
print(classification_report(y_test, y_pred3))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred3)
sns.heatmap(cm, annot=True, fmt='d', cmap='Reds')
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# ROC Curve and AUC
fpr, tpr, thresholds = roc_curve(y_test, y_pred3)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], 'r--')  # Diagonal line
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend(loc="lower right")
plt.show()

"""**Model Training using Support Vector Machine (SVM) classifier**"""

# SVC
#Hyperparameter tuning
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
svc= SVC(probability=True)

parameters = {
    'gamma': [0.0001, 0.001, 0.01, 0.1],
    'C':[0.01, 0.05, 0.5, 0.1, 1,10, 15,20]
}
grid_search = GridSearchCV(svc, parameters)
grid_search.fit(X_train, y_train)

grid_search.best_params_

grid_search.best_score_

svc = SVC(C=15, gamma=0.01, probability=True)
svc.fit(X_train, y_train)

y_pred4 = svc.predict(X_test)

y_pred4

# Model evaluation on trained data
train_score = svc.score(X_train, y_train)
print("Training Score:", train_score)

# Model evaluation on test data
# Calculate accuracy score
accuracy_svm = accuracy_score(y_test, y_pred4)
print("Accuracy:", accuracy_svm)

# Classification Report
print(classification_report(y_test, y_pred4))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred3)
sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges')
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# ROC Curve and AUC
fpr, tpr, thresholds = roc_curve(y_test, y_pred4)
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], 'r--')  # Diagonal line
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend(loc="lower right")
plt.show()

models = pd.DataFrame({
    'Model': ['Logistic Regression', 'Random Forest Classifier','KNN', 'SVM'],
    'Score': [100*round(accuracy_log,4), 100*round(accuracy_ranf,4), 100*round(accuracy_knn,4), 100*round(accuracy_svm,4)]
})
models.sort_values(by = 'Score', ascending = False)

"""**Ploting all AOC curve**"""

from sklearn import metrics
plt.figure(figsize=(8,5))
models = [
{
    'label': 'LR',
    'model': log,
},
{
    'label': 'RF',
    'model': rf,
},
{
    'label': 'KNN',
    'model': knn,
},
{
    'label': 'svm',
    'model': svc,
},
]
for m in models:
    model = m['model']
    model.fit(X_train, y_train)
    y_pred=model.predict(X_test)
    fpr1, tpr1, thresholds = metrics.roc_curve(y_test, model.predict_proba(X_test)[:,1])
    auc = metrics.roc_auc_score(y_test,model.predict(X_test))
    plt.plot(fpr1, tpr1, label='%s - ROC (area = %0.2f)' % (m['label'], auc))

plt.plot([0, 1], [0, 1],'r--')
plt.xlim([-0.01, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('1 - Specificity (False Positive Rate)', fontsize=12)
plt.ylabel('Sensitivity (True Positive Rate)', fontsize=12)
plt.title('ROC - Breast Cancer Prediction', fontsize=12)
plt.legend(loc="lower right", fontsize=12)
plt.savefig("roc_breast_cancer.jpeg", format='jpeg', dpi=400, bbox_inches='tight')

plt.show()

"""**Plotting Performance Evaluation**"""

from sklearn import metrics
import numpy as np
import matplotlib.pyplot as plt
models = [
{
    'label': 'LR',
    'model': log,
},
{
    'label': 'RF',
    'model': rf,
},
{
    'label': 'KNN',
    'model': knn,
},
{
    'label': 'SVM',
    'model': svc,
},
]

means_roc = []
means_accuracy = [100*round(accuracy_log,4), 100*round(accuracy_ranf,4), 100*round(accuracy_knn,4), 100*round(accuracy_svm,4)]

for m in models:
    model = m['model']
    model.fit(X_train, y_train)
    y_pred=model.predict(X_test)
    fpr1, tpr1, thresholds = metrics.roc_curve(y_test, model.predict_proba(X_test)[:,1])
    auc = metrics.roc_auc_score(y_test,model.predict(X_test))
    auc = 100*round(auc,4)
    means_roc.append(auc)

print(means_accuracy)
print(means_roc)

# data to plot
n_groups = 4
means_accuracy = tuple(means_accuracy)
means_roc = tuple(means_roc)

# create plot
fig, ax = plt.subplots(figsize=(8,5))
index = np.arange(n_groups)
bar_width = 0.35
opacity = 0.8

rects1 = plt.bar(index, means_accuracy, bar_width,
alpha=opacity,
color='mediumpurple',
label='Accuracy (%)')

rects2 = plt.bar(index + bar_width, means_roc, bar_width,
alpha=opacity,
color='rebeccapurple',
label='ROC (%)')

plt.xlim([-1, 8])
plt.ylim([70, 104])

plt.title('Performance Evaluation - Breast Cancer Prediction', fontsize=12)
plt.xticks(index, ('   LR', '  RF', '  KNN', '  SVM'), rotation=40, ha='center', fontsize=12)
plt.legend(loc="upper right", fontsize=10)
plt.savefig("PE_breast_cancer.jpeg", format='jpeg', dpi=400, bbox_inches='tight')
plt.show()

import pickle

# Save the model to a file using pickle
model_filename = 'svm_model.pkl'
with open(model_filename, 'wb') as file:
    pickle.dump(svc, file)
print(f"Model saved to {model_filename}")

# Load the model from the file
with open(model_filename, 'rb') as file:
    loaded_model = pickle.load(file)
print("Model loaded successfully")

# Test the loaded model
accuracy = loaded_model.score(X_test, y_test)
print(f"Accuracy of the loaded model: {accuracy * 100:.2f}%")

"""**Building a Predictive system**"""

input_data = (9.465,21.01,60.11,269.4,0.1044,0.07773,0.02172,0.01504,0.1717,0.06899,0.2351,2.011,1.66,14.2,0.01052,0.01755,0.01714,0.009333,0.02279,0.004237,10.41,31.56,67.03,330.7,0.1548,0.1664,0.09412,0.06517,0.2878,0.09211)

# to change the input data to a numpy array

input_data_as_numpy_array = np.asarray(input_data)

# Reshape the numpy array as we are predictiong for one datapoint
input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)

# make prediction
prediction = loaded_model.predict(input_data_reshaped)
print(prediction)

if (prediction[0] == 1):
  print('The Breast cancer is Malignant')

else:
  print('The Breast Cancer is Benign')

